<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: rails | Brandon Hilkert]]></title>
  <link href="http://brandonhilkert.com/blog/categories/rails/atom.xml" rel="self"/>
  <link href="http://brandonhilkert.com/"/>
  <updated>2016-12-29T15:40:02-05:00</updated>
  <id>http://brandonhilkert.com/</id>
  <author>
    <name><![CDATA[Brandon Hilkert]]></name>
    <email><![CDATA[brandonhilkert@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Monitoring Sidekiq Using AWS Lambda and Slack]]></title>
    <link href="http://brandonhilkert.com/blog/monitoring-sidekiq-using-aws-lambda-and-slack/"/>
    <updated>2016-10-25T11:54:00-04:00</updated>
    <id>http://brandonhilkert.com/blog/monitoring-sidekiq-using-aws-lambda-and-slack</id>
    <content type="html"><![CDATA[<p>It&rsquo;s no mystery I&rsquo;m a <a href="http://sidekiq.org/">Sidekiq</a> fan &mdash; my background job processing library of choice for any non-trivial applications. My favorite feature of Sidekiq has to be retries. By default, failed jobs will retry 25 times over the course of 21 days.</p>

<p>As a remote company, we use Slack to stay in touch with everyone AND to manage/monitor our infrastructure (hello #chatops). We can deploy from Slack (we don&rsquo;t generally, we have full CI) and be notified of infrastructure and application errors.</p>

<!--more-->


<p>When Sidekiq retries accumulate, it&rsquo;s a good indication that something more severe might be wrong. Rather than get an email we won&rsquo;t see for 30 minutes, we decided to integrate these notifications in to Slack. In doing so, we found <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> to be a lightweight solution to tie the monitoring of Sidekiq and notifications in Slack together.</p>

<h2>The Problem</h2>

<p><a href="https://www.bark.us/">Bark</a> is background job-heavy. The web application is a glorified CRUD app that sets up the data needed to poll a child&rsquo;s social media feed and monitor for potential issues. The best-case scenario for a parent is that they will never hear from us.</p>

<p>Because Bark&rsquo;s background jobs commonly interact with 3rd-party APIs, failures aren&rsquo;t a big surprise. APIs can be down, network connections can fail &mdash; Sidekiq&rsquo;s retry logic protects us from transient network errors. Under normal circumstances, jobs retry and ultimately run successfully after subsequent attempts. These are non-issues and something we don&rsquo;t need an engineer to investigate.</p>

<p>There are times when retries accumulate, giving us a strong indication that something more severe may be wrong. Initially, we setup New Relic to notify us of an increased error rate. This worked for simple cases, but was sometimes a false positive. As a result, we started to ignore them, which potentially masked more important issues.</p>

<p>We soon realized one of the gauges of application health was the number of retries in the Sidekiq queue. We have the Sidekiq Web UI mounted within our admin application, so we&rsquo;d browse there a few times a day to make sure the number of retries weren&rsquo;t outside our expectations (in this case &lt; 50 were acceptable).</p>

<p>This wasn&rsquo;t a great use of our time. Ideally, we wanted a Slack notification when the number of Sidekiq retries was > 50.</p>

<h2>The Solution</h2>

<p>Because Bark is on AWS, we naturally looked to their tools for assistance. In this case, we needed something that would poll Sidekiq, check the number of retries, and <code>POST</code> to Slack if the number of retries was > 50.</p>

<p>There were a few options:</p>

<ol>
<li>Add the Sidekiq polling and Slack notification logic to our main application and setup a Cron job</li>
<li>Create a new satellite application that ONLY does the above (microservices???)</li>
<li>Setup an AWS Lambda function to handle the above logic</li>
</ol>


<p>The first two options would&rsquo;ve worked, but I was hesistant to add complexity to our main application. I was also hesitant to have to manage another application (ie. updates, etc.) for something that seemed simple.</p>

<p>Option &ldquo;AWS Lambda&rdquo; won! Let&rsquo;s take a look at the implementation.</p>

<h3>Sidekiq Queue Data Endpoint</h3>

<p>First, we need to expose the number of Sideki retries somehow. As I mentioned above, the Sidekiq web UI is mounted in our admin application, but behind an authentication layer that would&rsquo;ve been non-trivial to publicly expose.</p>

<p>Instead, we created a new Rails route to respond with some basic details about the Sidekiq system.</p>

<p>```
require &lsquo;sidekiq/api&rsquo;</p>

<p>class SidekiqQueuesController &lt; ApplicationController
  skip_before_action :require_authentication</p>

<p>  def index</p>

<pre><code>base_stats = Sidekiq::Stats.new
stats = {
   enqueued: base_stats.enqueued,
   queues: base_stats.queues,
   busy: Sidekiq::Workers.new.size,
   retries: base_stats.retry_size
}

render json: stats
</code></pre>

<p>  end
end
```</p>

<p>along with the route:</p>

<p><code>
resources :sidekiq_queues, only: [:index]
</code></p>

<p>As you can see, the endpoint is public (there&rsquo;s no job args or names exposed &mdash; just counts). The code digs in to the <a href="https://github.com/mperham/sidekiq/wiki/API">Sidekiq API</a> to interrogate the size of queues.</p>

<h3>Slack Incoming WebHook</h3>

<p>We want to be able to POST to Slack when the number of Sidekiq retries are > 50. To do this, we&rsquo;ll setup a custom incoming webhook integration in Slack.</p>

<p>We&rsquo;ll start by choose <code>Apps &amp; integrations</code> from within the main Slack options. From here, choose <code>Manage</code> in the top right, and then <code>Custom Integrations</code> on the left. You&rsquo;ll have 2 options:</p>

<ol>
<li>Incoming WebHooks</li>
<li>Slash Commands</li>
</ol>


<p>We&rsquo;ll choose <code>Incoming Webhooks</code> and choose <code>Add Configuration</code> to add a new one. From here, we&rsquo;ll supply the information needed to specify the channel where the notifications will appear and how they look.</p>

<p>The most important of this step is to get the <code>Webhook URL</code>. This will be the URL we <code>POST</code> to from within our Lambda function when retries are above our acceptable threshold.</p>

<h3>AWS Lambda Function</h3>

<p>Now that we have our endpoint to expose the number of retries (among other things) and the Slack webhook URL to <code>POST</code> to, we need to setup the AWS Lambda function to tie to the two together. We&rsquo;ll start by creating a new Lambda function with the defaults &mdash; using the latest Node.</p>

<p>For the trigger, we&rsquo;ll use &ldquo;CloudWatch Events &ndash; Schedule&rdquo;:</p>

<p><img class="center" src="/images/sidekiq-monitor/lambda-trigger.png" title="&ldquo;AWS Lambda trigger&rdquo;" ></p>

<p>From here, we&rsquo;ll enter a name and description for our rule and define its rate (I chose every 5 minutes). Enable the trigger and we&rsquo;ll move to defining our code. Next, we&rsquo;ll give the function a name and choose the latest NodeJS as the runtime. Within the inline editor, we&rsquo;ll use the following code:</p>

<p>```
var AWS = require(&lsquo;aws-sdk&rsquo;);
var url = require(&lsquo;url&rsquo;);
var https = require(&lsquo;https&rsquo;);
var sidekiqURL, hookUrl, slackChannel, retryThreshold;</p>

<p>sidekiqUrl = &lsquo;[Sidekiq queue JSON endpoint]&rsquo;
hookUrl = &lsquo;[Slack Incoming WebHooks URL w/ token]&rsquo;;
slackChannel = &lsquo;#operations&rsquo;;  // Enter the Slack channel to send a message to
retryThreshold = 50;</p>

<p>var postMessageToSlack = function(message, callback) {</p>

<pre><code>var body = JSON.stringify(message);
var options = url.parse(hookUrl);
options.method = 'POST';
options.headers = {
    'Content-Type': 'application/json',
    'Content-Length': Buffer.byteLength(body),
};

var postReq = https.request(options, function(res) {
    var chunks = [];
    res.setEncoding('utf8');
    res.on('data', function(chunk) {
        return chunks.push(chunk);
    });
    res.on('end', function() {
        var body = chunks.join('');
        if (callback) {
            callback({
                body: body,
                statusCode: res.statusCode,
                statusMessage: res.statusMessage
            });
        }
    });
    return res;
});

postReq.write(body);
postReq.end();
</code></pre>

<p>};</p>

<p>var getQueueStats = function(callback) {</p>

<pre><code>var options = url.parse(sidekiqUrl);
options.headers = {
    'Accept': 'application/json',
};

var getReq = https.request(options, function(res){
    var body = '';

    res.setEncoding('utf8');

    //another chunk of data has been recieved, so append it to `str`
    res.on('data', function (chunk) {
        body += chunk;
    });

    //the whole response has been recieved, so we just print it out here
    res.on('end', function () {
        if (callback) {
            callback({
                body: JSON.parse(body),
                statusCode: res.statusCode,
                statusMessage: res.statusMessage
            });
        }
    });
})

getReq.end();
</code></pre>

<p>}</p>

<p>var processEvent = function(event, context) {</p>

<pre><code>getQueueStats(function(stats){
    console.log('STATS: ', stats.body);

    var retries = stats.body.retries;

    if (retries &gt; retryThreshold) {
        var slackMessage = {
            channel: slackChannel,
            text: "www Sidekiq retries - " + retries
        };

        postMessageToSlack(slackMessage, function(response) {
            if (response.statusCode &lt; 400) {
                console.info('Message posted successfully');
                context.succeed();
            } else if (response.statusCode &lt; 500) {
                console.error("Error posting message to Slack API: " + response.statusCode + " - " + response.statusMessage);
                context.succeed();  // Don't retry because the error is due to a problem with the request
            } else {
                // Let Lambda retry
                context.fail("Server error when processing message: " + response.statusCode + " - " + response.statusMessage);
            }
        });
    } else {
        console.info('Sidekiq retries were ' + retries + ' . Below threshold.');
        context.succeed();
    }
})
</code></pre>

<p>};</p>

<p>exports.handler = function(event, context) {</p>

<pre><code>processEvent(event, context);
</code></pre>

<p>};
```</p>

<p><em>Note: <code>sidekiqURL</code> and <code>hookURL</code> need to be defined with appropriate values for this to work.</em></p>

<p>Review and save the Lambda function and we&rsquo;re all set!</p>

<h3>Review</h3>

<p>We can review the Lambda function logs on CloudWatch. Go to CloudWatch and choose &ldquo;Logs&rdquo; from the left menu. From here, we&rsquo;ll click the link to the name of our Lambda function:</p>

<p><img class="center" src="/images/sidekiq-monitor/sidekiq-logs.png" title="&ldquo;AWS Cloudwatch logs&rdquo;" ></p>

<p>From here, logs for each invocation of the Lambda function will be grouped in to a log stream:</p>

<p><img class="center" src="/images/sidekiq-monitor/log-streams.png" title="&ldquo;AWS Cloudwatch log streams&rdquo;" ></p>

<p>Grouped by time, each link will contain multiple invocations. A single execution is wrapped with a <code>START</code> and <code>END</code>, as shown in the logs. Messages in between will be calls to <code>console.log</code> from within our function. We logged the results of the Sidekiq queue poll for debugging purposes, so you can see that below:</p>

<p><img class="center" src="/images/sidekiq-monitor/log.png" title="&ldquo;AWS Cloudwatch log&rdquo;" ></p>

<p>This was invocation where the number of retries were &lt; 50, and a result, didn&rsquo;t need to <code>POST</code> to Slack. Let&rsquo;s take a look at the opposite:</p>

<p><img class="center" src="/images/sidekiq-monitor/log-post.png" title="&ldquo;AWS Cloudwatch log posting to Slack&rdquo;" ></p>

<p>We can see the <code>Message posted successfully</code> log indicating our message was successfully sent to Slack&rsquo;s incoming webhook.</p>

<p>Finally, here&rsquo;s what the resulting message looks like in Slack when the number of Sidekiq retries are above our threshold:</p>

<p><img class="center" src="/images/sidekiq-monitor/slack.png" title="&ldquo;Slack notifications for Sidekiq retries&rdquo;" ></p>

<h2>Conclusion</h2>

<p>Using new tools is fun, but not when it brings operational complexity. I&rsquo;ve personally found AWS lamba to be a great place for endpoints/functionality that feels cumbersome to include in my applications. Bringing these notifications in to Slack has been a big win for our team. We took a previously untrustworthy notification (NewRelic error rate) and brought some clarity to the state and health of our applications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rails Progress Indicator for Turbolinks Using Nprogress]]></title>
    <link href="http://brandonhilkert.com/blog/rails-progress-indicator-for-turbolinks-using-nprogress/"/>
    <updated>2016-04-29T09:44:00-04:00</updated>
    <id>http://brandonhilkert.com/blog/rails-progress-indicator-for-turbolinks-using-nprogress</id>
    <content type="html"><![CDATA[<p>Contrary to popular opinion, I&rsquo;m a fan of <a href="https://github.com/turbolinks/turbolinks">Turbolinks</a>.
I leave it enabled in all my Rails applications. Most of the negative opinions I hear relate to it &ldquo;breaking&rdquo; third-party jQuery plugins. I say &ldquo;breaking&rdquo; because it&rsquo;s not <em>really</em> changing the plugin&rsquo;s behavior &mdash; just requires the plugin to be initialized differently.</p>

<!--more-->


<p>If you&rsquo;re upgrading to a newer version of Rails and have a bunch of legacy JavaScript code, I can imagine this being difficult. But if you&rsquo;re green-fielding a new application, there&rsquo;s no reason not to take advantage of it. I wrote extensively about <a href="http://brandonhilkert.com/blog/organizing-javascript-in-rails-application-with-turbolinks/">how to organize JavaScript in a Rails application with Turbolinks enabled</a>. If you&rsquo;re struggling to get your JavaScript code to work as expected on clicks through the application, take a look at that post. I continue to use that organization pattern for all my applications and it never lets me down.</p>

<p>With Turbolinks enabled, interacting with an application feels smooth and fast. No more full page refreshes.</p>

<p>Every once in awhile we&rsquo;ll stumble on a page request takes longer than others. Rather than having the user sit there thinking nothing is happening, we can offer better feedback through a loading progress bar, specifically <a href="http://ricostacruz.com/nprogress/">nprogress</a>. I&rsquo;ve found it to be the perfect companion to Turbolinks to create a great user experience.</p>

<h2>The Problem</h2>

<p>In a traditional web application, when we click a link or submit a form, we get a loading spinner where the favicon typically appears. We might also see text in the status bar saying &ldquo;Connecting&hellip;&rdquo; or &ldquo;Loading&hellip;&rdquo;. These are the loading indications that internet users have become accustomed to.</p>

<p>By adopting Turbolinks, we not longer get those loading feedback mechanisms because the request for the new page is asynchronous. Once the request is complete, the new content is rendered in place of the previous page&rsquo;s body element. For fast page loads, this isn&rsquo;t a problem. However, if you have applications like mine, every once in awhile, you might have a page request take a few seconds (reasons for this are beyond the scope of this article). In those cases, a user might click a link and sit there for 2-3 sec. without any indication the page is loading. While Turbolinks generally improves the user experience of our application, having no user feedback for several seconds is not ideal (ideally, you&rsquo;d want to address a page request that takes multiple seconds). This is where <code>nprogress</code> can help.</p>

<h2>The Solution</h2>

<p><a href="http://ricostacruz.com/nprogress/"><code>nprogress</code></a> is a progress loading indicator, like what you see on YouTube.</p>

<p>Like other JavaScript libraries, there&rsquo;s <a href="https://github.com/caarlos0/nprogress-rails">a Ruby Gem that vendors the code and includes it in the Rails asset pipeline</a>.</p>

<p>We&rsquo;ll first add <code>nprogress-rails</code> to our Gemfile:</p>

<p><code>
gem "nprogress-rails"
</code></p>

<p>Bundle to install the new gem:</p>

<p><code>
$ bundle install
</code></p>

<p>Now with <code>nprogress</code> installed, we need to include the JavaScript in our application. We&rsquo;ll do this by adding the following the <code>app/assets/javascripts/application.js</code> manifest:</p>

<p><code>
//= require nprogress
//= require nprogress-turbolinks
</code></p>

<p>We first include the <code>nprogress</code> JavaScript source, and then an adapter that&rsquo;ll hook the Turbolinks request to the progress indicator.</p>

<p><em>Note: If you&rsquo;re familiar with Turbolinks and its events, you&rsquo;ll recognize the <a href="https://github.com/caarlos0/nprogress-rails/blob/master/app/assets/javascripts/nprogress-turbolinks.js">events triggered</a>.</em></p>

<p>By default, the <code>nprogress</code> loading bar is anchored to the top of the browser window, but we need to include some CSS to make this work. Let&rsquo;s open the <code>app/assets/stylesheets/application.scss</code> manifest file and add the following:</p>

<p><code>
*= require nprogress
*= require nprogress-bootstrap
</code></p>

<p><em>Note: Including <code>nprogress-bootstrap</code> isn&rsquo;t necessary if you don&rsquo;t use <a href="http://getbootstrap.com/css/">Bootstrap</a> in your application. I typically do, so I&rsquo;m going to include it.</em></p>

<p>At this point, we&rsquo;ll have a working loading indicator. But what if we want to tweak the styles to match your application&rsquo;s theme?</p>

<h2>Customizing Nprogress Styles</h2>

<p>Because the <a href="https://github.com/caarlos0/nprogress-rails/blob/master/app/assets/stylesheets/nprogress.scss#L1"><code>nprgress</code> styles are Sass</a>, we can overwrite the variables for customization.</p>

<p>There are 3 variables available to overwite:</p>

<ul>
<li><code>$nprogress-color</code></li>
<li><code>$nprogress-height</code></li>
<li><code>$nprogress-zindex</code></li>
</ul>


<p>For <a href="https://www.bark.us/">Bark</a>, we have an aqua accent color with use throughout the site. It made sense for the <code>nprogress</code> loading indicator to be that same color.</p>

<p>Back in our <code>app/assets/stylesheets/application.scss</code>, I overwrote the variable before including the <code>nprogress</code> source code:</p>

<p>```
$nprogress-color: #37c8c9;</p>

<p>@import &ldquo;nprogress&rdquo;;
@import &ldquo;nprogress-bootstrap&rdquo;;
```</p>

<h2>Summary</h2>

<p>I&rsquo;ve found <code>nprogress</code> to be a great companion library to Turbolinks. The two libraries together provide a much better user experience over full page refreshes. Turbolinks helps asynchronously load the page content that&rsquo;s changing and <code>nprogress</code> gives the user feedback that their request is in progress. Now, even when a user has to suffer through multi-second page loads, at least they&rsquo;ll know it&rsquo;s not broken and don&rsquo;t have to click again.</p>

<p>The <a href="https://github.com/turbolinks/turbolinks/blob/master/src/turbolinks/progress_bar.coffee">latest version of Turbolinks has a progress bar
built-in</a>.
I&rsquo;m looking forward to removing the dependency if it performs similarly.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sidekiq As A Microservice Message Queue]]></title>
    <link href="http://brandonhilkert.com/blog/sidekiq-as-a-microservice-message-queue/"/>
    <updated>2015-11-30T12:06:00-05:00</updated>
    <id>http://brandonhilkert.com/blog/sidekiq-as-a-microservice-message-queue</id>
    <content type="html"><![CDATA[<p>In the recent series on transitioning to microservices, I detailed a path to move a large legacy Rails monolith to a cluster of a dozen microservices. But not everyone starts out with a legacy monolith. In fact, given Rails popularity amongst startups, <strong>it&rsquo;s likely most Rails applications don&rsquo;t live to see 4+ years in production</strong>. So what if we don&rsquo;t have a huge monolith on our hands? Are microservices still out of the question?</p>

<p>Sadly, the answer is, &ldquo;it depends&rdquo;. The &ldquo;depends&rdquo; part is specific to your context. While microservices may seem like the right move for you and your application, it&rsquo;s also possible it could cause a mess if not done carefully.</p>

<!--more-->


<p>This post will explore opportunities for splitting out unique microservices using <a href="http://sidekiq.org/">Sidekiq</a>, without introducing an enterprise message broker like <a href="https://www.rabbitmq.com/">RabbitMQ</a> or <a href="http://kafka.apache.org/">Apache Kafka</a>.</p>

<h2>When are Microservices right?</h2>

<p>Martin Fowler <a href="http://martinfowler.com/articles/microservice-trade-offs.html">wrote about trade-offs that come when introducing microservices</a>.</p>

<p>The article outlines 6 pros and cons introduced when you moved a microservices-based architecture. The strongest argument for microservices is the strengthening of module boundaries.</p>

<p>Module boundaries are naturally strengthened when we&rsquo;re forced to move code to another codebase. The result being, in most cases a group of microservices appears to be better constructed than the legacy monolith it was extracted from.</p>

<p>There&rsquo;s no doubt Rails allows developers to get something up and running very quickly. Sadly, you can do so while making a big mess at the same time. It&rsquo;s worth noting there&rsquo;s nothing stopping a monolith from being well constructed. With some discipline, <a href="https://www.youtube.com/watch?v=KJVTM7mE1Cc">your monolith can be the bright and shiny beauty that DHH wants it to be</a>.</p>

<h2>Sidekiq Queues</h2>

<p>Ok, ok. You get it. Microservices can be awesome, but they can also make a big mess. I want to tell you about how I recently avoided a big mess without going &ldquo;all in&rdquo;.</p>

<p>There&rsquo;s no hiding I&rsquo;m a huge <a href="http://sidekiq.org/">Sidekiq</a> fan. It&rsquo;s my goto solution for background processing.</p>

<p>Sidekiq has the notion of <a href="https://github.com/mperham/sidekiq/wiki/Advanced-Options#workers">named queues</a> for both <a href="https://github.com/mperham/sidekiq/wiki/Advanced-Options#workers">jobs</a> and <a href="https://github.com/mperham/sidekiq/wiki/Advanced-Options#queues">workers</a>. This is great from the standpoint that it allows you to put that unimportant long-running job in a different queue without delayed other important fast-running jobs.</p>

<p>A typical worker might look like:</p>

<p>```
class ImportantWorker
  include Sidekiq::Worker</p>

<p>  def perform(id)</p>

<pre><code># Do the important stuff
</code></pre>

<p>  end
end
```</p>

<p>If we want to send this job to a different queue, we&rsquo;d add <code>sidekiq_options queue: :important</code> to the worker, resulting in:</p>

<p>```
class ImportantWorker
  include Sidekiq::Worker
  sidekiq_options queue: :important</p>

<p>  def perform(id)</p>

<pre><code># Do the important stuff
</code></pre>

<p>  end
end
```</p>

<p>Now, we need to make sure the worker process that&rsquo;s running the jobs knows to process jobs off this queue. A typical worker might be invoked with:</p>

<p><code>
bin/sidekiq
</code></p>

<p>Since new jobs going through this worker will end up on the <code>important</code> queue, we want to make sure the worker is processing jobs from the <code>important</code> queue too:</p>

<p><code>
bin/sidekiq -q important -q default
</code></p>

<p><em>Note: Jobs that don&rsquo;t specify a queue will go to the <code>default</code> queue. We have to include the <code>default</code> queue when we using the <code>-q</code> option, otherwise the default queue will be ignored in favor of the queue passed to the <code>-q</code> option.</em></p>

<p>The best part, you don&rsquo;t even have to have multiple worker processes to process jobs from multiple queues. Furthermore, the <code>important</code> queue can be checked twice as often as the <code>default</code> queue:</p>

<p><code>
bin/sidekiq -q important,2 -q default
</code></p>

<p>This flexibility of where jobs are enqueued and how they&rsquo;re processed gives us an incredible amount of freedom when building our applications.</p>

<h2>Extracting Worker to a Microservice</h2>

<p>Let&rsquo;s assume that we&rsquo;ve deployed your main application to Heroku. The application uses Sidekiq and we&rsquo;ve included a Redis add-on. With the addition of the add-on, our application now has a <code>REDIS_URL</code> environment variable that Sidekiq connects to on startup. We have a web process, and worker process. A pretty standard Rails stack:</p>

<p><img class="center" src="/images/sidekiq/rails-web-worker.png" title="&ldquo;Rails with typical worker process&rdquo;" ></p>

<p><strong>What&rsquo;s stopping us from using that same <code>REDIS_URL</code> in another application?</strong></p>

<p>Nothing, actually. And if we consider what we know about the isolation of jobs in queue and workers working on specific queues, there&rsquo;s nothing stopping us from having workers for a specific queue in a different application altogether.</p>

<p>Remember <code>ImportantWorker</code>, imagine the logic for that job was better left for a different application. We&rsquo;ll leave that part a little hand-wavey because there still should be a really good reason to do so. But we&rsquo;ll assume you&rsquo;ve thought long and hard about this and decided the core application was not a great place for this job logic.</p>

<p>Extracting the worker a separate application might now look something like this:</p>

<p><img class="center" src="/images/sidekiq/rails-with-microservice.png" title="&ldquo;Using Sidekiq as a Message Queue between two Rails microservices&rdquo;" ></p>

<h2>Enqueueing Jobs with the Sidekiq Client</h2>

<p>Typically, to enqueue the <code>ImportantWorker</code> above, we&rsquo;d call the following from our application:</p>

<p><code>
ImportantWorker.perform_async(1)
</code></p>

<p>This works great when <code>ImportantWorker</code> is defined in our application. With the expanded stack above, <code>ImportantWorker</code> now lives in a new microservice, which means we don&rsquo;t have access to the <code>ImportantWorker</code> class from within the application. We <em>could</em> define it in the application just so we can enqueue it, with the intent that the application won&rsquo;t process jobs for that worker, but that feels funny to me.</p>

<p>Rather, we can turn to the underlying Sidekiq client API to enqueue the job instead:</p>

<p>```
Sidekiq::Client.push(
  &ldquo;class&rdquo; => &ldquo;ImportantWorker&rdquo;,
  &ldquo;queue&rdquo; => &ldquo;important&rdquo;,
  &ldquo;args&rdquo; => [1]
)</p>

<p>```</p>

<p><em>Note: We have to be sure to define the <code>class</code> as a string <code>"ImportantWorker"</code>, otherwise we&rsquo;ll get an exception during enqueuing because the worker isn&rsquo;t defined in the application.</em></p>

<h2>Processing Sidekiq Jobs from a Microservice</h2>

<p>Now we&rsquo;re pushing jobs to the <code>important</code> queue, but have nothing in our application to process them. In fact, our worker process isn&rsquo;t even looking at that queue:</p>

<p><code>
bin/sidekiq -q default
</code></p>

<p>From our microservice, we setup a worker process to <strong>ONLY</strong> look at the <code>important</code> queue:</p>

<p><code>
bin/sidekiq -q important
</code></p>

<p>We define the <code>ImportantWorker</code> in our microservice:</p>

<p>```
class ImportantWorker
  include Sidekiq::Worker
  sidekiq_options queue: :important</p>

<p>  def perform(id)</p>

<pre><code># Do the important stuff
</code></pre>

<p>  end
end
```</p>

<p>And now when the worker picks jobs out of the <code>important</code> queue, it&rsquo;ll process them using the <code>ImportantWorker</code> defined above in our microservice.</p>

<p>If we wanted to go one step further, the microservice could then enqueue a job using the Sidekiq client API to a queue that only the core application is working on in order to send communication back the other direction.</p>

<h2>Summary</h2>

<p>Any architectural decision has risks. Microservices are no exception. Microservices can be easier than an enterprise message broker, cluster of new servers and a handful of devops headaches.</p>

<p>I originally dubbed this the &ldquo;poor man&rsquo;s message bus&rdquo;. With more thought, there&rsquo;s nothing &ldquo;poor&rdquo; about this. Sidekiq has a been a reliable piece of our infrastructure and I have no reason to believe that&rsquo;ll change, even if we are using it for more than just simple background processing from a single application.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Path to Services - Part 3 - Synchronous Events]]></title>
    <link href="http://brandonhilkert.com/blog/a-path-to-services-part-3-synchronous-events/"/>
    <updated>2015-10-15T09:07:00-04:00</updated>
    <id>http://brandonhilkert.com/blog/a-path-to-services-part-3-synchronous-events</id>
    <content type="html"><![CDATA[<p><em>This article was originally posted on the <a href="http://plumbing.pipelinedeals.com/">PipelineDeals Engineering
Blog</a></em></p>

<p>In the <a href="/blog/a-path-to-services-part-1-start-small/">previous article in this series</a>, we introduced a billing service to determine which features an account could access. If you remember, <a href="/our-path-to-services-part-1-start-small/">the email service</a> was a &ldquo;fire and forget&rdquo; operation and was capable of handling throughput delays given its low value to the core application.</p>

<p>This post will explore how we handle synchronous communication for a service like billing where an inline response is required to service a request from the core application.</p>

<!--more-->


<h2>Background</h2>

<p>If you remember from the previous post, we introduced the billing service to an infrastructure that looked like this:</p>

<p><img class="center" src="/images/services/app-email-billing.png" title="&ldquo;Web application with Email and Billing Microservice&rdquo;" ></p>

<p>Handling multiple pricing tiers in a SaaS app means you have to control authorization based on account status. Our billing service encapsulates the knowledge of which features correspond to which pricing tier.</p>

<p>For instance, one feature is the ability to send trackable email to contacts in your PipelineDeals account. To service this request, we add an option to the bulk action menu from a list view:</p>

<p><img class="center" src="/images/services/send-email.png" title="&ldquo;Send email feature&rdquo;" ></p>

<h2>Service Request</h2>

<p>Before we can conditionally show this option based on the pricing tier, we have to first make a request to the billing service to get the list of features available to that user.</p>

<p>```
class Billing::Features
  def initialize(user)</p>

<pre><code>@user = user
@account = user.account
</code></pre>

<p>  end</p>

<p>  def list</p>

<pre><code>Rails.cache.fetch("account_#{account.id}_billing_features") do
  response = Billing::Api.get "account/#{account.id}/features"
  response['features']
end
</code></pre>

<p>  end</p>

<p>  private</p>

<p>  attr_reader :user, :account
end
```</p>

<p><code>Billing::Api</code>, in this case, is a wrapper around the API calls to handle exceptions and other information like security.</p>

<p><em>Note: When making synchronous HTTP calls like this, it&rsquo;s worth considering the failure state and providing a default response set in that case so the user isn&rsquo;t burdened with a failure page. In this case, one option would be dumb down the features on the page to the most basic tier.</em></p>

<h2>Serving a JSON API</h2>

<p>Plenty of articles have been written about how to create a JSON API with Rails, so we won&rsquo;t rehash those techniques here. Instead, we&rsquo;ll highlight patterns we&rsquo;ve used for consistency.</p>

<p>We tend to reserve the root URL namespace for UI-related routes, so we start by creating a unique namespace for the API:</p>

<p>```
namespace :api do
  resources :account do</p>

<pre><code>resource :features, only: :show
</code></pre>

<p>  end
end
```</p>

<p>This setup gives us the path <code>/api/account/:account_id/features</code>. We haven&rsquo;t found a need for versioning internal APIs. If we decided to in the future, we could always add the API version as a request header.</p>

<p>The <code>features</code> endpoint looks like:</p>

<p>```
class Api::FeaturesController &lt; Api::ApiController
  skip_before_filter :verify_authenticity_token</p>

<p>  def show</p>

<pre><code>render json: {
  success: true,
  features: AccountFeatures.new(@account_id).list
}
</code></pre>

<p>  end
end
```</p>

<p>Notice <code>Api::FeaturesController</code> inherits from <code>Api::ApiController</code>. We keep the API-related functionality in this base controller so each endpoint will get access to security and response handling commonalities.</p>

<p><code>AccountFeatures</code> is a PORO that knows how to list billing features for a particular account. We could&rsquo;ve queried it straight from an ActiveRecord-based model, but our handling of features is a little more complicated than picking them straight from the database.</p>

<p>Another note here is that we haven&rsquo;t introduced a serializing library like <code>active_model_serializers</code> or <code>jbuilder</code>. Using <code>render json</code> alone has serviced us well for simple APIs. We reach for something more complex when the response has more attributes than shown above.</p>

<h2>Handling Service Response</h2>

<p>By introducing <code>Rails.cache</code>, we can serve requests (after the initial) without requiring a call to the billing service.</p>

<p>One of the first things we do is serialize the set of features to JavaScript so our client-side code has access:</p>

<p><code>
&lt;%= javascript_tag do %&gt;
  window.Features = &lt;%= Billing::Features.new(logged_in_user).list.to_json %&gt;;
&lt;% end %&gt;
</code></p>

<p>We also include a helper module in to our Rails views/controllers, so we can handle conditional feature logic:</p>

<p>```
module Features
  def feature_enabled?(feature)</p>

<pre><code>Billing::Features.new(logged_in_user).list.include?(feature.to_s)
</code></pre>

<p>  end
end
```</p>

<h2>Synchronous Side Effects</h2>

<p>When we <a href="/our-path-to-services-part-2-synchronous-vs-asynchronous/">looked at asynchronous service requests</a>, there was less immediacy associated with the request due to its &ldquo;fire-and-forget&rdquo; nature. A synchronous request, on the other hand, will handle all requests to the core application, so scaling can be challenge and infrastructure costs can add up.</p>

<p><img class="center" src="/images/services/synchronous-service-cost.png" title="&ldquo;Increased cost by introducing synchronous microservice&rdquo;" ></p>

<p>In addition to the infrastructure costs, performance can be a factor. If the original page response time was 100ms and we&rsquo;re adding a synchronous service request that takes another 100ms, all of a sudden we&rsquo;ve doubled our users' response times. And while this architectural decision might seem like an optimization, I&rsquo;m positive none of our users will thank us for making their page load times 2x slower.</p>

<h2>Summary</h2>

<p>As you can see, there&rsquo;s little magic to setting up a synchronous service request.</p>

<p>Challenges appear when you consider failure states at every point in the service communication &ndash; the service could be down, or the HTTP request itself could fail due to network connectivity. As mentioned above, providing a default response during service failure is a great start to increasing the application&rsquo;s reliability. Optionally, <a href="https://en.wikipedia.org/wiki/Circuit_breaker_design_pattern">the circuit break pattern</a> can provide robust handling of network failures.</p>

<p>Part 4 in this series will cover how we manage asynchronous communication between services, specifically around an <a href="https://github.com/PipelineDeals/mantle">open source gem we built called Mantle</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Path to Services - Part 2 - Synchronous vs. Asynchronous]]></title>
    <link href="http://brandonhilkert.com/blog/a-path-to-services-part-2-synchronous-vs-asynchronous/"/>
    <updated>2015-08-14T10:32:00-04:00</updated>
    <id>http://brandonhilkert.com/blog/a-path-to-services-part-2-synchronous-vs-asynchronous</id>
    <content type="html"><![CDATA[<p><em>This article was originally posted on the <a href="http://plumbing.pipelinedeals.com/">PipelineDeals Engineering
Blog</a></em></p>

<p>In the <a href="/blog/a-path-to-services-part-1-start-small/">previous article in this series</a>, we moved the responsibility of emails to a separate Rails application. In order to leverage this new service, we created a PORO to encapsulate the specifics of communicating with our new service by taking advantage of Sidekiq&rsquo;s built-in retry mechanism to protect from intermittent network issues.</p>

<p>Communication between microservices can be broken down in to 2 categories: <strong>synchronous</strong> and  <strong>asynchronous</strong>.
Understanding when to use each is critical in maintaining a healthy infrastructure. This post will explore details about these two methods of communication and their associated use cases.</p>

<!--more-->


<h2>Background</h2>

<p>Continuing the discussion of our architecture from last time, we have a primary
Rails web application serving the majority of our business logic. We now have an additional application that&rsquo;s only responsibility is the formatting and sending of emails.</p>

<p><img class="center" src="/images/services/app-email.png" title="&ldquo;Application service with email microservice&rdquo;" ></p>

<p>In this article, we&rsquo;ll discuss the addition of our Billing service. The service&rsquo;s responsibility to is to process transactions related to money. This can come in the form of a trial conversion, adding a seat to an additional account, or deleting users from an existing account, among others.</p>

<p>Like many SaaS applications, PipelineDeals has multiple tiers of service. The most expensive intended for customers needing advanced functionality. Part of the billing service&rsquo;s responsibility is to manage the knowledge of which features an account can access.</p>

<p>So stepping back to the main PipelineDeals web application, the app has to decide which features to render at page load. Because the billing service is our source of truth for this information, a page load will now require a call to this service to understand which features to render.</p>

<p>This new dependency looks a little different than the email dependency from the
<a href="/blog/a-path-to-services-part-1-start-small/">previous article</a>. Email has the
luxury of not being in the dependency path of a page load. Very few customers
will complain if an email is 10 seconds late. On the other hand, they&rsquo;ll
complain immediately if their account won&rsquo;t load, and rightfully so.</p>

<p><img class="center" src="/images/services/app-email-billing.png" title="&ldquo;Application service include email and billing microservices&rdquo;" ></p>

<p>An interesting benefit from having already extracted the email service is that the billing service sends email regarding financial transactions and account changes. Typically, we would have done the same thing for every other Rails app that needed to send email, which was integrate <code>ActionMailer</code> and setup the templates and mailers needed to do the work. In this case, we can add those emails to the email service and use the same communication patterns we do from the main web application to trigger the sending of an email from the billing service. This does require making changes to 2 different projects for a single feature (business logic in billing and mailer in email), but removes the necessity to configure another app to send email properly. We viewed this as a benefit.</p>

<h2>Asynchronous Events</h2>

<p>As the easier of the two, asynchronous would be any communication not necessary for the request/response cycle to complete. Email is the perfect example. Logging also falls in to this category.</p>

<p>For the networks gurus out there, this would be similar to UDP communication. More of a fire-and-forget approach.</p>

<p>An email, in this case, is triggered due to something like an account sign up.
We send a welcome email thanking the customer for signing up and giving them
some guidance on how to get the most benefit from the application. Somewhere in
the process of signing up, the code triggers an email and passes along the data
needed for email template.</p>

<p>As shown in the previous article, the call to send the email looks something like this:</p>

<p><code>ruby
Email.to current_user, :user_welcome
</code></p>

<p>The value in this call is that under the covers, it&rsquo;s enqueuing a Sidekiq job:</p>

<p><code>ruby
EmailWorker.perform_async(opts)
</code></p>

<p>where <code>opts</code> is a hash of data related to the email and the variables needed for the template.</p>

<p><em>Note: Because the options are serialized to JSON, values in hash must be simple structures. Objects won&rsquo;t work here.</em></p>

<p>As you can see above, the code invoking the <code>Email.to</code> method doesn&rsquo;t care about what it returns. In fact, it doesn&rsquo;t return anything we care about at this point. So as long as the method is called, the code can move forward without waiting for the email to finish sending.</p>

<p>Extracting asynchronous operations like this that exist in a code path is a
great way to improve performance. There are times, though, where deferring an operation to background job might not make sense.</p>

<p>For example, imagine a user changes the name of a person. They click one of their contact&rsquo;s names, enter a new name, and click &ldquo;Save&rdquo;. It doesn&rsquo;t make sense to send the task of updating the actual name in the database to a background job because depending on what else is in the queue at that time, the update might not complete until after the next refresh, which would make the user believe their update wasn&rsquo;t successful. This would be incredibly confusing.</p>

<p>Logging is another perfect candidate for asynchronicity. In most cases, our users don&rsquo;t care if a log of their actions has been written to the database before their next refresh. It&rsquo;s information we may want to store, and as a result, can be a fire-and-forget operation. We can rest easy knowing we&rsquo;ll have that information, soon-ish, and it won&rsquo;t add any additional overhead to the end user&rsquo;s request cycle.</p>

<p>The opposite of asynchronous events like this are <strong>synchronous</strong> events! (surprise right?). Let&rsquo;s explore how they&rsquo;re different.</p>

<h2>Synchronous Events</h2>

<p>We can look at synchronous events as dependencies of the request cycle. We use MySQL as a backend for the main PipelineDeals web application and queries to MySQL would be considered synchronous. In that, in order to successfully fulfill the current request, we require the information returned from MySQL before we can respond.</p>

<p>In most cases, we don&rsquo;t think of our main datastore as a service. It doesn&rsquo;t necessarily have a separate application layer on top of it, but it&rsquo;s behavior and requirements are very much like a service.</p>

<p>If we consider the addition of our billing service above, we require information about the features allowed for a particular account before we can render the page. This allows us to include/exclude modules they should or should not see. The flow goes something like this:</p>

<p><code>Web request -&gt; lookup account in DB -&gt; Request features from Billing service -&gt; render page</code></p>

<p>If the request to the billing service didn&rsquo;t require a response, we would consider this to be an <strong>asynchronous</strong> service, which might change how we invoke the request for data.</p>

<p>Synchronous communication can happen over a variety of protocols. The most
common is a JSON payload over HTTP. In general, it&rsquo;s not the most performant,
but it&rsquo;s one of the easier to debug and is human-readable, so it tends to be pretty popular.</p>

<p>The synchronous services we&rsquo;ve setup all communicate over HTTP. Rails APIs are a known thing. We&rsquo;re familiar with the stack and the dependencies required to set up a common JSON API, which is a large part of the reason it&rsquo;s our preferred communication protocol between services.</p>

<h2>Summary</h2>

<p>We&rsquo;ve simplified the communication between services into these two categories. Knowing this helps dictate the infrastructure and configuration of the applications.</p>

<p>Next time, we&rsquo;ll take a closer look at the synchronous side and the specifics about the JSON payloads involved to send an email successfully.</p>
]]></content>
  </entry>
  
</feed>
